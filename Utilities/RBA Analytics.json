[ {
  "defaultTab" : "output",
  "description" : "\n\n**Description**\n\nThe Automation Job is designed to collect and analyze execution data from a specified API, providing detailed metrics regarding job performance and return on investment (ROI). This process involves several key actions:\n\n- **Data Collection**: Connect to an API endpoint to retrieve project and job execution details.\n- **Data Filtering**: Filter the execution data based on a specified date range.\n- **ROI Calculation**: Compute execution time, successful and failed executions, and derive ROI metrics such as time and cost savings.\n- **Data Summary**: Generate and display a summarized report highlighting total execution times and cost savings.\n\n**Prerequisites**\n\n- Python 3 runtime is required for executing the script.\n- Access to the specified API endpoints with appropriate credentials.\n- The necessary Python libraries such as `requests`, `pandas`, and `tabulate` should be installed.\n\n**Options**\n\n| Option                    | Description                                                                                          | Default                           |\n|---------------------------|------------------------------------------------------------------------------------------------------|-----------------------------------|\n| `format`                  | Output Format. Options: `text` to render a text table, `html` to render an HTML table.               | `text`                            |\n| `instance`                | Instance URL, e.g., `https://<DEMO>.runbook.pagerduty.cloud`.                                        | `https://demo.runbook.pagerduty.cloud` |\n| `projects`                | Project to analyze, or leave blank for all projects.                                                 | `\"global-production\"`             |\n| `apikey`                  | Secure API Key for Automation Server.                                                                |                                   |\n| `lastxdays`               | Number of days to look back from today to fetch executions.                                          | `7`                               |\n\n**Any external endpoints used**\n\nThe Automation Job interacts with the following external endpoints:\n- API base URL for Rundeck: `https://demo.runbook.pagerduty.cloud/api/44`\n- API base URL for ROI data: `<instance>/api/50`\n\n**Disclaimer**\n\nThis document and the described automation job are provided \"as is\" without warranty of any kind, express or implied. The user assumes all risks associated with the use of this job. The creators or contributors are not liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this job, even if advised of the possibility of such damage.\n",
  "executionEnabled" : true,
  "group" : "Utilities",
  "id" : "b6b87ed6-d6e4-4576-8fc4-bdff175d5ec8",
  "loglevel" : "INFO",
  "multipleExecutions" : true,
  "name" : "RBA Analytics",
  "nodeFilterEditable" : false,
  "nodefilters" : {
    "dispatch" : {
      "excludePrecedence" : true,
      "keepgoing" : false,
      "rankOrder" : "ascending",
      "successOnEmptyNodeFilter" : false,
      "threadcount" : "1"
    },
    "filter" : ".*rundeckers-a.*"
  },
  "nodesSelectedByDefault" : true,
  "options" : [ {
    "description" : "Output Format.\n\n- text (render text table)\n- html (render html table)",
    "enforced" : true,
    "hidden" : true,
    "label" : "Output Format",
    "name" : "format",
    "type" : "text",
    "value" : "html",
    "values" : [ "text", "html" ],
    "valuesListDelimiter" : ","
  }, {
    "description" : "Instance.  \nEg. https://<DEMO>.runbook.pagerduty.cloud",
    "label" : "Instance",
    "name" : "instance",
    "required" : true,
    "type" : "text",
    "value" : "<insert your RBA Cluster URL here>"
  }, {
    "description" : "Project to analyze\n\neg \"project-1-home\"\n\nYou can add more as follows:\n\"project-1-home\",\"project2-home\",\"project3-home\"\n\nor leave blank for all projects.\n",
    "label" : "Project",
    "name" : "projects",
    "type" : "text",
    "value" : "\"<insert your prject name here>\""
  }, {
    "description" : "Secure API Key for Automation Server",
    "hidden" : true,
    "label" : "API Key",
    "name" : "apikey",
    "secure" : true,
    "storagePath" : "keys/path/to/api/key",
    "type" : "text",
    "valueExposed" : true
  }, {
    "description" : "How many days to look back from",
    "label" : "Last x Days from today",
    "name" : "lastxdays",
    "required" : true,
    "value" : "7"
  } ],
  "plugins" : {
    "ExecutionLifecycle" : {
      "roi-metrics" : {
        "userRoiData" : "[{\"key\":\"min\",\"label\":\"Minutes\",\"value\":\"30\",\"desc\":\"Minutes saved in manual work (Field key: min)\"}]"
      }
    }
  },
  "runnerSelector" : {
    "filter" : "US-WEST",
    "runnerFilterMode" : "TAGS",
    "runnerFilterType" : "TAG_FILTER_AND"
  },
  "scheduleEnabled" : true,
  "schedules" : [ ],
  "sequence" : {
    "commands" : [ {
      "autoSecureInput" : "false",
      "description" : "Collect Data",
      "fileExtension" : "py",
      "interpreterArgsQuoted" : false,
      "passSecureInput" : "false",
      "plugins" : {
        "LogFilter" : [ {
          "config" : {
            "datatype" : "text/x-markdown",
            "sanitizeHtml" : "true",
            "striped" : "false"
          },
          "type" : "render-datatype"
        } ]
      },
      "script" : "# MIT License\n# \n# Copyright (c) 2024\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport requests\nimport json\nimport os\nimport re\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom tabulate import tabulate\n\n# Configuration variables\nDAYS_RANGE = \"@option.lastxdays@\"  # Number of days to look back for fetching executions\nNUM_DAYS = int(DAYS_RANGE)\nBASE_URL = \"@option.instance@/api/44\"  # API base URL for Rundeck\nROI_BASE_URL = \"@option.instance@/api/50\"  # API base URL for ROI data\nAPI_TOKEN = \"@option.apikey@\"  # API token\nTEST_PROJECTS = [@option.projects@]  # Set to None to process all projects\nJOB_NAME_FILTER = None  # Set to a regex pattern to filter job names, or None to include all\nOUTPUT_FORMAT = 'html'  # Output format: 'text' or 'html'\nROI_FIELDS = [\"min\"]  # Specify the ROI fields you want to include\nORDER_BY = \"Average Execution Time (s)\"  # Specify the friendly name of the column to sort by\nAVERAGE_SALARY_PER_HOUR_STR = \"50.0\"  # Average salary per hour as a string value\n\nHEADERS = {\n    'Accept': 'application/json',\n    'X-Rundeck-Auth-Token': API_TOKEN\n}\n\n# Calculate dynamic date range\nEND_DATE = datetime.today()\nSTART_DATE = END_DATE - timedelta(days=NUM_DAYS)\n\n# Function to convert AVERAGE_SALARY_PER_HOUR to float\ndef get_average_salary_per_hour(salary_str):\n    try:\n        return float(salary_str)\n    except ValueError:\n        raise ValueError(f\"Invalid salary input '{salary_str}'. Please provide a valid numeric value.\")\n\n# Convert the salary string to a float\nAVERAGE_SALARY_PER_HOUR = get_average_salary_per_hour(AVERAGE_SALARY_PER_HOUR_STR)\n\ndef get_projects():\n    url = f\"{BASE_URL}/projects\"\n    response = requests.get(url, headers=HEADERS)\n    if response.status_code != 200:\n        print(f\"Failed to fetch projects: {response.status_code}\")\n        return []\n    return response.json()\n\ndef get_jobs(project):\n    url = f\"{BASE_URL}/project/{project}/jobs\"\n    response = requests.get(url, headers=HEADERS)\n    if response.status_code != 200:\n        print(f\"Failed to fetch jobs for project {project}: {response.status_code}\")\n        return []\n    return response.json()\n\ndef get_job_executions(job_id, start_date=None, end_date=None):\n    executions = []\n    offset = 0\n    limit = 50  # Increased limit for efficiency\n\n    while True:\n        url = f\"{BASE_URL}/job/{job_id}/executions?offset={offset}&max={limit}\"\n        response = requests.get(url, headers=HEADERS)\n        if response.status_code != 200:\n            print(f\"Failed to fetch executions for job {job_id}: {response.status_code}\")\n            return []\n\n        data = response.json()\n        if 'paging' in data and 'executions' in data:\n            executions.extend(data['executions'])\n            paging = data['paging']\n            offset += limit\n            if offset >= paging['total']:\n                break\n        else:\n            print(f\"Unexpected response format for executions: {data}\")\n            return []\n\n    # Filter executions based on date range\n    filtered_executions = [\n        execution for execution in executions\n        if isinstance(execution, dict) and\n        start_date <= datetime.fromtimestamp(execution['date-started']['unixtime'] / 1000) <= end_date\n    ]\n\n    return filtered_executions\n\ndef get_execution_roi_data(execution_id):\n    url = f\"{ROI_BASE_URL}/execution/{execution_id}/roimetrics/data\"\n    response = requests.get(url, headers=HEADERS)\n    if response.status_code != 200:\n        return {}  # Return empty dict if no ROI data is available\n    return response.json()\n\ndef fetch_all_executions():\n    all_projects = get_projects()\n    projects = [project for project in all_projects if project['name'] in TEST_PROJECTS] if TEST_PROJECTS else all_projects\n    all_executions = []\n\n    # Compile the job name regex pattern if provided\n    job_name_pattern = re.compile(JOB_NAME_FILTER) if JOB_NAME_FILTER else None\n\n    for project in projects:\n        project_name = project['name']\n        jobs = get_jobs(project_name)\n\n        # Filter jobs by JOB_NAME_FILTER regex if provided\n        if job_name_pattern:\n            jobs = [job for job in jobs if job_name_pattern.search(job['name'])]\n\n        for job in jobs:\n            job_name = job['name']\n            job_id = job['id']\n            executions = get_job_executions(job_id, START_DATE, END_DATE)\n\n            for execution in executions:\n                execution_id = execution['id']\n                roi_data = get_execution_roi_data(execution_id)  # Fetch ROI data\n\n                project = execution['job']['project']\n                user = execution['user']\n                status = execution['status']\n                start_time = datetime.fromtimestamp(execution['date-started']['unixtime'] / 1000)\n\n                # Handle cases where date-ended might be missing\n                end_time = (\n                    datetime.fromtimestamp(execution['date-ended']['unixtime'] / 1000)\n                    if 'date-ended' in execution and 'unixtime' in execution['date-ended']\n                    else None\n                )\n\n                # Calculate total duration if end_time exists, otherwise set to None\n                total_duration = (end_time - start_time).total_seconds() if end_time else None\n\n                execution_data = {\n                    'Project': project,\n                    'Job Name': job_name,\n                    'User': user,\n                    'Status': status,\n                    'Total Duration (s)': total_duration\n                }\n\n                # Add ROI data to execution_data\n                if roi_data:\n                    for key, value in roi_data.items():\n                        if key in ROI_FIELDS:\n                            # Convert numeric fields to float if possible\n                            try:\n                                value = float(value)\n                            except (ValueError, TypeError):\n                                pass\n                            execution_data[key] = value\n\n                all_executions.append(execution_data)\n\n    return pd.DataFrame(all_executions)\n\ndef generate_summary(df):\n    if df.empty:\n        return df, 0, 0, 0\n\n    # Identify ROI columns present in the data\n    known_columns = {'Project', 'Job Name', 'User', 'Status', 'Total Duration (s)'}\n    roi_columns = [col for col in df.columns if col in ROI_FIELDS]\n\n    # Add 'Successful' and 'Failed' columns\n    df['Successful'] = df['Status'] == 'succeeded'\n    df['Failed'] = df['Status'] == 'failed'\n\n    # Group and summarize data\n    agg_dict = {\n        'Total Duration (s)': ['sum', 'mean'],\n        'Successful': 'sum',\n        'Failed': 'sum',\n    }\n\n    for roi_field in roi_columns:\n        agg_dict[roi_field] = ['first']  # Since the ROI value is the same for all executions\n\n    summary_df = df.groupby(['Project', 'Job Name']).agg(agg_dict)\n\n    # Flatten MultiIndex columns\n    summary_df.columns = [' '.join(col).strip() if col[1] else col[0] for col in summary_df.columns.values]\n    summary_df = summary_df.reset_index()\n\n    # Rename columns to friendly names\n    rename_dict = {\n        'Total Duration (s) sum': 'Total Execution Time (s)',\n        'Total Duration (s) mean': 'Average Execution Time (s)',\n        'Successful sum': 'Successful Executions',\n        'Failed sum': 'Failed Executions'\n    }\n\n    for roi_field in roi_columns:\n        rename_dict[f\"{roi_field} first\"] = f\"{roi_field.capitalize()} per Execution (minutes)\"\n\n    summary_df = summary_df.rename(columns=rename_dict)\n\n    # Calculate the total ROI value as per-execution ROI value multiplied by the number of successful executions\n    for roi_field in roi_columns:\n        per_execution_col = f\"{roi_field.capitalize()} per Execution (minutes)\"\n        total_roi_col = f\"Total {roi_field.capitalize()} Value (minutes)\"\n        summary_df[total_roi_col] = (\n            summary_df[per_execution_col] * summary_df['Successful Executions']\n        )\n\n    # Calculate Cost Associated with Time Saved\n    # First, convert timesaved from minutes to hours\n    per_execution_col = \"Timesaved per Execution (minutes)\"\n    total_roi_col = \"Total Timesaved Value (minutes)\"\n\n    if per_execution_col in summary_df.columns:\n        # Calculate Cost Saved per Execution\n        summary_df[\"Cost Saved per Execution ($)\"] = (\n            (summary_df[per_execution_col] / 60) * AVERAGE_SALARY_PER_HOUR\n        ).round(2)\n    else:\n        summary_df[\"Cost Saved per Execution ($)\"] = 0\n\n    if total_roi_col in summary_df.columns:\n        # Calculate Total Cost Saved\n        summary_df[\"Total Cost Saved ($)\"] = (\n            (summary_df[total_roi_col] / 60) * AVERAGE_SALARY_PER_HOUR\n        ).round(2)\n    else:\n        summary_df[\"Total Cost Saved ($)\"] = 0\n\n    # Round numerical columns\n    numeric_cols = summary_df.select_dtypes(include='number').columns\n    summary_df[numeric_cols] = summary_df[numeric_cols].round(2)\n\n    # Include jobs without ROI data\n    # Merge with all unique jobs to ensure all jobs are included\n    all_jobs = df[['Project', 'Job Name']].drop_duplicates()\n    summary_df = pd.merge(all_jobs, summary_df, on=['Project', 'Job Name'], how='left')\n\n    # Fill NaN values for ROI columns\n    roi_related_cols = [f\"{roi_field.capitalize()} per Execution (minutes)\" for roi_field in roi_columns] + \\\n                       [f\"Total {roi_field.capitalize()} Value (minutes)\" for roi_field in roi_columns] + \\\n                       [\"Cost Saved per Execution ($)\", \"Total Cost Saved ($)\"]\n\n    for col in roi_related_cols:\n        if col in summary_df.columns:\n            summary_df[col] = summary_df[col].fillna(0)\n\n    # Fill NaN values for other columns\n    summary_df['Successful Executions'] = summary_df['Successful Executions'].fillna(0).astype(int)\n    summary_df['Failed Executions'] = summary_df['Failed Executions'].fillna(0).astype(int)\n    summary_df['Total Execution Time (s)'] = summary_df['Total Execution Time (s)'].fillna(0)\n    summary_df['Average Execution Time (s)'] = summary_df['Average Execution Time (s)'].fillna(0)\n\n    # Order by the specified column if it exists\n    if ORDER_BY in summary_df.columns:\n        # Handle cases where ORDER_BY column may have empty strings; replace with 0 for sorting\n        summary_df[ORDER_BY] = pd.to_numeric(summary_df[ORDER_BY], errors='coerce').fillna(0)\n        summary_df = summary_df.sort_values(by=ORDER_BY, ascending=False)\n    else:\n        print(f\"Warning: ORDER_BY column '{ORDER_BY}' not found in summary. Skipping sorting.\")\n\n    # Compute totals and append totals row\n    numeric_cols = summary_df.select_dtypes(include='number').columns\n    totals = summary_df[numeric_cols].sum()\n    totals_row = pd.DataFrame(totals).T\n    totals_row.index = ['Total']\n    # For non-numeric columns, set to empty or appropriate value\n    for col in summary_df.columns:\n        if col not in totals_row.columns:\n            if col == 'Job Name':\n                totals_row[col] = 'Total'\n            else:\n                totals_row[col] = ''\n\n    # Reorder columns\n    totals_row = totals_row[summary_df.columns]\n    # Append totals_row to summary_df using pd.concat()\n    summary_df = pd.concat([summary_df, totals_row], ignore_index=False)\n\n    # Now create MultiIndex columns to add 'ROI' header over ROI columns\n    # Prepare the list of tuples for MultiIndex columns\n    columns = []\n    for col in summary_df.columns:\n        if col in roi_related_cols:\n            columns.append(('ROI', col))\n        else:\n            columns.append(('', col))\n    summary_df.columns = pd.MultiIndex.from_tuples(columns)\n\n    # Extract total Timesaved, Moneysaved, and Cost Saved values from totals\n    total_timesaved = totals.get(f\"Total Timesaved Value (minutes)\", 0)\n    total_costsaved = totals.get(f\"Total Moneysaved Value (minutes)\", 0)\n    total_cost_saved = totals.get(\"Total Cost Saved ($)\", 0) if totals.get(\"Total Cost Saved ($)\") is not None else 0\n\n    return summary_df, total_timesaved, total_costsaved, total_cost_saved\n\ndef display_results(df, summary_df, total_timesaved, total_costsaved, total_cost_saved):\n    if df.empty:\n        print(\"No execution data to display.\")\n        return\n\n    total_jobs = len(df['Job Name'].unique())\n    total_projects = len(df['Project'].unique())\n    total_successful = df['Successful'].sum()\n    total_failed = df['Failed'].sum()\n\n    # Added emojis and formatting to highlight the summary\n    print(\"\\n🚀 Automation Job Digest 🚀\")\n    print(\"\\n===============================\")\n    print(f\"\\n📅 Date Range: Last {DAYS_RANGE} Days\")\n    print(f\"Average Salary per Hour: ${AVERAGE_SALARY_PER_HOUR}\")\n\n    print(f\"\\n📊 Total Projects: {total_projects}\")\n    print(f\"📋 Total Jobs: {total_jobs}\")\n    print(f\"✅ Successful Executions: {int(total_successful)}\")\n    print(f\"❌ Failed Executions: {int(total_failed)}\")\n\n    # Highlight total Timesaved, Costsaved, and Cost Saved with emojis\n    print(f\"\\n⏱️ Total Timesaved (minutes): {total_timesaved}\")\n    print(f\"💵 Total Cost Associated with Time Saved ($): {total_cost_saved}\")\n\n    if OUTPUT_FORMAT == 'text':\n        # Flatten MultiIndex columns for text display\n        summary_df.columns = ['\\n'.join(col).strip() if col[0] else col[1] for col in summary_df.columns.values]\n        print(\"\\n📈 Summary Table:\\n\")\n        print(tabulate(summary_df, headers='keys', tablefmt='grid', showindex=False))\n    elif OUTPUT_FORMAT == 'html':\n        print(summary_df.to_html(index=False))\n\n# Main logic to execute the script\ndf = fetch_all_executions()\nif not df.empty:\n    summary_df, total_timesaved, total_costsaved, total_cost_saved = generate_summary(df)\n    display_results(df, summary_df, total_timesaved, total_costsaved, total_cost_saved)\nelse:\n    print(\"No executions found in the specified date range.\")",
      "scriptInterpreter" : "python -u"
    } ],
    "keepgoing" : false,
    "strategy" : "node-first"
  },
  "tags" : "analytics",
  "uuid" : "b6b87ed6-d6e4-4576-8fc4-bdff175d5ec8"
} ]
